import os, json, random, yaml, torch
from datasets import load_dataset          # å¦‚æœåé¢è¦ç”¨ï¼Œå¯ä»¥å…ˆä¿ç•™
from transformers import AutoModelForCausalLM, AutoTokenizer

cfg = yaml.safe_load(open("config.yaml"))

tok = AutoTokenizer.from_pretrained(cfg["tokenizer"], trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-1.3b-base",
    torch_dtype=torch.float16,             # ä¸¤è¡Œæ”¾åˆ° GPU
    device_map="auto",                     # è®© accelerate è‡ªè¡Œåˆ‡ gpu
    trust_remote_code=True,
)

# å¯é€‰ï¼šåŠ è½½ LoRA / Adapter
# model.load_adapter("adapter/adapter.zip")

os.environ["TOKENIZERS_PARALLELISM"] = "false"

print("Loading eval dataâ€¦")
data = json.load(open("data/fim_dataset.json"))

def em(pred: str, tgt: str) -> int:
    """exactâ€‘match"""
    return int(pred.strip() == tgt.strip())


def run_once(seed: int = 42) -> float:
    random.seed(seed)
    sample = random.sample(data, 6)

    ems = []
    for ex in sample:
        # DeepSeekâ€‘Coder è‡ªå¸¦çš„ FIM special tokens ä¸ OpenAI é£æ ¼ä¸€è‡´
        prompt = f"<|fim_begin|>{ex['prefix']}<|fim_hole|>{ex['suffix']}<|fim_end|>"
        inputs = tok(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            pred_ids = model.generate(
                **inputs,
                do_sample=True,
                temperature=0.1,
                top_p=1.0,
                # ğŸ”¹ presence_penalty ä¸æ˜¯ HF å‚æ•°ï¼Œæ¢æˆ repetition_penalty
                repetition_penalty=1.05,
                max_new_tokens=cfg["max_tokens"],
            )

        # å¯¹ FIM ä»»åŠ¡åªä¿ç•™è¡¥å…¨æ®µè½ï¼ˆå»æ‰ promptï¼‰
        pred = tok.decode(pred_ids[0], skip_special_tokens=True)
        ems.append(em(pred, ex["target"]))
    return sum(ems) / len(sample)


scores = [run_once(i) for i in range(3)]
print("Best EM:", max(scores))
